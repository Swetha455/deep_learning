{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0ac0746e",
      "metadata": {
        "id": "0ac0746e"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install tensorflow opencv-python-headless --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3aed3a80",
      "metadata": {
        "id": "3aed3a80"
      },
      "outputs": [],
      "source": [
        "# Create directories\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data/videos\", exist_ok=True)\n",
        "os.makedirs(\"data/captions\", exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1106a8bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "1106a8bb",
        "outputId": "95b045d7-6c2b-44d7-8716-43bf0e46dda8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload `map.txt`, `train.txt`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-eb5340db-28f8-4519-8e11-53eb7ead3f2c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-eb5340db-28f8-4519-8e11-53eb7ead3f2c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving map.txt to map.txt\n",
            "Saving train.txt to train.txt\n"
          ]
        }
      ],
      "source": [
        "# Upload the four MSVD caption files\n",
        "from google.colab import files\n",
        "print(\"Upload `map.txt`, `train.txt`\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Save them into data/captions\n",
        "import shutil\n",
        "for fn in uploaded.keys():\n",
        "    shutil.move(fn, f\"data/captions/{fn}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4a3db0b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4a3db0b7",
        "outputId": "66a50ecc-546e-47a3-8f31-4f1c6c0a0d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your video clips...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6fb71b38-75f9-4fcb-953b-895bdb5c1200\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6fb71b38-75f9-4fcb-953b-895bdb5c1200\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving -wa0umYJVGg_23_41.avi to -wa0umYJVGg_23_41.avi\n",
            "Saving -wa0umYJVGg_100_115.avi to -wa0umYJVGg_100_115.avi\n",
            "Saving -wa0umYJVGg_117_123.avi to -wa0umYJVGg_117_123.avi\n",
            "Saving -wa0umYJVGg_139_157.avi to -wa0umYJVGg_139_157.avi\n",
            "Saving -wa0umYJVGg_168_176.avi to -wa0umYJVGg_168_176.avi\n",
            "Saving -wa0umYJVGg_271_276.avi to -wa0umYJVGg_271_276.avi\n",
            "Saving -wa0umYJVGg_286_290.avi to -wa0umYJVGg_286_290.avi\n",
            "Saving 05gNigkqfNU_24_32.avi to 05gNigkqfNU_24_32.avi\n",
            "Saving 05gNigkqfNU_25_34.avi to 05gNigkqfNU_25_34.avi\n",
            "Saving 05gNigkqfNU_78_84.avi to 05gNigkqfNU_78_84.avi\n",
            "Saving 05gNigkqfNU_11_23.avi to 05gNigkqfNU_11_23.avi\n",
            "Saving -_hbPLsZvvo_5_8.avi to -_hbPLsZvvo_5_8.avi\n",
            "Saving -_hbPLsZvvo_18_25.avi to -_hbPLsZvvo_18_25.avi\n",
            "Saving -_hbPLsZvvo_19_25.avi to -_hbPLsZvvo_19_25.avi\n",
            "Saving -_hbPLsZvvo_19_26.avi to -_hbPLsZvvo_19_26.avi\n",
            "Saving -_hbPLsZvvo_43_55.avi to -_hbPLsZvvo_43_55.avi\n",
            "Saving -_hbPLsZvvo_49_55.avi to -_hbPLsZvvo_49_55.avi\n",
            "Saving -_hbPLsZvvo_172_179.avi to -_hbPLsZvvo_172_179.avi\n",
            "Saving -_hbPLsZvvo_211_219.avi to -_hbPLsZvvo_211_219.avi\n",
            "Saving -_hbPLsZvvo_269_275.avi to -_hbPLsZvvo_269_275.avi\n",
            "Saving -_hbPLsZvvo_288_305.avi to -_hbPLsZvvo_288_305.avi\n",
            "Saving -_hbPLsZvvo_323_328.avi to -_hbPLsZvvo_323_328.avi\n",
            "Saving 0hyZ__3YhZc_632_637.avi to 0hyZ__3YhZc_632_637.avi\n",
            "Saving 0hyZ__3YhZc_279_283.avi to 0hyZ__3YhZc_279_283.avi\n",
            "Saving 0hyZ__3YhZc_289_295.avi to 0hyZ__3YhZc_289_295.avi\n",
            "Saving 0hyZ__3YhZc_302_307.avi to 0hyZ__3YhZc_302_307.avi\n",
            "Saving 0hyZ__3YhZc_341_348.avi to 0hyZ__3YhZc_341_348.avi\n",
            "Saving 0hyZ__3YhZc_352_356.avi to 0hyZ__3YhZc_352_356.avi\n",
            "Saving 0hyZ__3YhZc_364_370.avi to 0hyZ__3YhZc_364_370.avi\n",
            "Saving 0hyZ__3YhZc_380_384.avi to 0hyZ__3YhZc_380_384.avi\n",
            "Saving 0hyZ__3YhZc_388_394.avi to 0hyZ__3YhZc_388_394.avi\n",
            "Saving 0hyZ__3YhZc_410_417.avi to 0hyZ__3YhZc_410_417.avi\n",
            "Saving 0hyZ__3YhZc_418_424.avi to 0hyZ__3YhZc_418_424.avi\n",
            "Saving 0hyZ__3YhZc_485_490.avi to 0hyZ__3YhZc_485_490.avi\n",
            "Saving 0hyZ__3YhZc_562_568.avi to 0hyZ__3YhZc_562_568.avi\n",
            "Saving 0hyZ__3YhZc_575_580.avi to 0hyZ__3YhZc_575_580.avi\n",
            "Saving 0hyZ__3YhZc_598_603.avi to 0hyZ__3YhZc_598_603.avi\n",
            "Saving 0lh_UWF9ZP4_103_110.avi to 0lh_UWF9ZP4_103_110.avi\n",
            "Saving 0lh_UWF9ZP4_138_145.avi to 0lh_UWF9ZP4_138_145.avi\n",
            "Saving 0lh_UWF9ZP4_148_155.avi to 0lh_UWF9ZP4_148_155.avi\n",
            "Saving 0lh_UWF9ZP4_157_160.avi to 0lh_UWF9ZP4_157_160.avi\n",
            "Saving 0lh_UWF9ZP4_165_170.avi to 0lh_UWF9ZP4_165_170.avi\n",
            "Saving 0lh_UWF9ZP4_174_178.avi to 0lh_UWF9ZP4_174_178.avi\n",
            "Saving 0lh_UWF9ZP4_178_182.avi to 0lh_UWF9ZP4_178_182.avi\n",
            "Saving 0lh_UWF9ZP4_183_190.avi to 0lh_UWF9ZP4_183_190.avi\n",
            "Saving 0lh_UWF9ZP4_191_197.avi to 0lh_UWF9ZP4_191_197.avi\n",
            "Saving 0lh_UWF9ZP4_199_207.avi to 0lh_UWF9ZP4_199_207.avi\n",
            "Saving 0lh_UWF9ZP4_215_226.avi to 0lh_UWF9ZP4_215_226.avi\n",
            "Saving 0k1Ak8aTMVI_4_12.avi to 0k1Ak8aTMVI_4_12.avi\n",
            "Saving 0lh_UWF9ZP4_21_26.avi to 0lh_UWF9ZP4_21_26.avi\n",
            "Saving 0lh_UWF9ZP4_27_31.avi to 0lh_UWF9ZP4_27_31.avi\n",
            "Saving 0lh_UWF9ZP4_38_46.avi to 0lh_UWF9ZP4_38_46.avi\n",
            "Saving 0lh_UWF9ZP4_50_60.avi to 0lh_UWF9ZP4_50_60.avi\n",
            "Saving 0lh_UWF9ZP4_62_69.avi to 0lh_UWF9ZP4_62_69.avi\n",
            "Saving 0lh_UWF9ZP4_71_77.avi to 0lh_UWF9ZP4_71_77.avi\n",
            "Saving 0lh_UWF9ZP4_79_82.avi to 0lh_UWF9ZP4_79_82.avi\n",
            "Saving 0lh_UWF9ZP4_82_87.avi to 0lh_UWF9ZP4_82_87.avi\n",
            "Saving 0lh_UWF9ZP4_94_103.avi to 0lh_UWF9ZP4_94_103.avi\n"
          ]
        }
      ],
      "source": [
        "# Upload videos (batch upload recommended)\n",
        "print(\"Upload your video clips...\")\n",
        "uploaded_videos = files.upload()\n",
        "\n",
        "for fn in uploaded_videos.keys():\n",
        "    shutil.move(fn, f\"data/videos/{fn}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def map_captions(map_file, train_file, output_file):\n",
        "    \"\"\"Maps video IDs in train.txt to original video names using map.txt.\n",
        "\n",
        "    Args:\n",
        "        map_file: Path to the map.txt file.\n",
        "        train_file: Path to the train.txt file.\n",
        "        output_file: Path to the output file.\n",
        "    \"\"\"\n",
        "    video_map = {}\n",
        "    with open(map_file, 'r') as f:\n",
        "        for line in f:\n",
        "            video_id, video_name = line.strip().split()\n",
        "            video_map[video_id] = video_name\n",
        "\n",
        "    with open(train_file, 'r') as f_in, open(output_file, 'w') as f_out:\n",
        "        for line in f_in:\n",
        "            parts = line.strip().split()\n",
        "            video_id = parts[0]\n",
        "            caption = \" \".join(parts[1:])  # Combine caption parts\n",
        "\n",
        "            if video_id in video_map:\n",
        "                f_out.write(f\"{video_map[video_id]} {caption}\\n\")\n",
        "            else:\n",
        "                print(f\"Warning: Video ID '{video_id}' not found in map.txt\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "map_captions(\"data/captions/map.txt\", \"data/captions/train.txt\", \"data/captions/mapped_train.txt\")\n"
      ],
      "metadata": {
        "id": "oEdz6IFRinj6"
      },
      "id": "oEdz6IFRinj6",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CAPTION DICTIONARY"
      ],
      "metadata": {
        "id": "0_aJWSXlvg1i"
      },
      "id": "0_aJWSXlvg1i"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def read_captions_from_file(file_path):\n",
        "    captions_dict = {}\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split(' ', 1)  # Split into 2 parts: video ID and caption\n",
        "            if len(parts) == 2:\n",
        "                video_id, caption = parts\n",
        "                # If the video ID already exists, append the caption to the list\n",
        "                if video_id in captions_dict:\n",
        "                    captions_dict[video_id].append(caption)\n",
        "                else:\n",
        "                    captions_dict[video_id] = [caption]\n",
        "\n",
        "    return captions_dict\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/data/captions/mapped_train.txt\"  # Replace with the actual path to your text file\n",
        "captions_dict = read_captions_from_file(file_path)\n",
        "\n",
        "# Save the dictionary to a JSON file\n",
        "output_file = \"captions_dict.json\"  # Specify the output file path\n",
        "with open(output_file, 'w') as json_file:\n",
        "    json.dump(captions_dict, json_file, indent=4)\n",
        "\n",
        "print(f\"Captions saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CYvNV_jvfwT",
        "outputId": "a02e13fc-d2c4-4ede-efcd-61303bae47a2"
      },
      "id": "2CYvNV_jvfwT",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captions saved to captions_dict.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEATURE EXTRACTION"
      ],
      "metadata": {
        "id": "RCO0LdsztQyA"
      },
      "id": "RCO0LdsztQyA"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "# Load pre-trained CNN (VGG16)\n",
        "base_model = VGG16(weights=\"imagenet\")\n",
        "model = Model(inputs=base_model.input, outputs=base_model.get_layer(\"fc2\").output)\n",
        "\n",
        "# Frame extraction\n",
        "def extract_frames(video_path, num_frames=10):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "    frames = []\n",
        "\n",
        "    for idx in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frame = cv2.resize(frame, (224, 224))\n",
        "            frame = img_to_array(frame)\n",
        "            frame = preprocess_input(frame)\n",
        "            frames.append(frame)\n",
        "    cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "# Feature extractor\n",
        "def extract_video_features(video_path):\n",
        "    frames = extract_frames(video_path)\n",
        "    if len(frames) == 0:\n",
        "        return None\n",
        "    features = model.predict(frames, verbose=0)\n",
        "    return np.mean(features, axis=0)\n",
        "\n",
        "# Paths\n",
        "input_video_folder = \"/content/data/videos\"\n",
        "output_feature_folder = \"/content/data/features\"\n",
        "os.makedirs(output_feature_folder, exist_ok=True)\n",
        "\n",
        "# Loop through videos\n",
        "for video_file in os.listdir(input_video_folder):\n",
        "    if video_file.endswith(\".mp4\"):\n",
        "        video_path = os.path.join(input_video_folder, video_file)\n",
        "        video_id = os.path.splitext(video_file)[0]\n",
        "        print(f\"Extracting from {video_file}...\")\n",
        "        features = extract_video_features(video_path)\n",
        "        if features is not None:\n",
        "            np.save(os.path.join(output_feature_folder, f\"{video_id}.npy\"), features)\n"
      ],
      "metadata": {
        "id": "VpzHcyz7tQaz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44571579-8f0d-456a-c273-778ccabd1fda"
      },
      "id": "VpzHcyz7tQaz",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m553467096/553467096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features for all captioned videos\n",
        "video_features = {}\n",
        "missing_files = []\n",
        "\n",
        "for video_file in captions_dict.keys():\n",
        "    path = f\"data/videos/{video_file}.avi\"  # Ensure extension matches your dataset\n",
        "    print(f\"Extracting features from {video_file}...\")\n",
        "    feats = extract_video_features(path)\n",
        "    if feats is not None:\n",
        "        video_features[video_file] = feats\n",
        "    else:\n",
        "        print(f\"Failed to extract features for {video_file}\")\n",
        "        missing_files.append(video_file)\n",
        "\n",
        "print(f\"Extracted features for {len(video_features)} videos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcLoWtwTpgv3",
        "outputId": "a548e01b-9d1d-4a9c-ff38-e381858bb8f6"
      },
      "id": "JcLoWtwTpgv3",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting features from -_hbPLsZvvo_172_179...\n",
            "Extracting features from -_hbPLsZvvo_18_25...\n",
            "Extracting features from -_hbPLsZvvo_19_25...\n",
            "Extracting features from -_hbPLsZvvo_19_26...\n",
            "Extracting features from -_hbPLsZvvo_211_219...\n",
            "Extracting features from -_hbPLsZvvo_269_275...\n",
            "Extracting features from -_hbPLsZvvo_288_305...\n",
            "Extracting features from -_hbPLsZvvo_323_328...\n",
            "Extracting features from -_hbPLsZvvo_43_55...\n",
            "Extracting features from -_hbPLsZvvo_49_55...\n",
            "Extracting features from -_hbPLsZvvo_5_8...\n",
            "Extracting features from -wa0umYJVGg_100_115...\n",
            "Extracting features from -wa0umYJVGg_117_123...\n",
            "Extracting features from -wa0umYJVGg_139_157...\n",
            "Extracting features from -wa0umYJVGg_168_176...\n",
            "Extracting features from -wa0umYJVGg_23_41...\n",
            "Extracting features from -wa0umYJVGg_271_276...\n",
            "Extracting features from -wa0umYJVGg_286_290...\n",
            "Extracting features from 05gNigkqfNU_11_23...\n",
            "Extracting features from 05gNigkqfNU_24_32...\n",
            "Extracting features from 05gNigkqfNU_25_34...\n",
            "Extracting features from 05gNigkqfNU_78_84...\n",
            "Extracting features from 0hyZ__3YhZc_279_283...\n",
            "Extracting features from 0hyZ__3YhZc_289_295...\n",
            "Extracting features from 0hyZ__3YhZc_302_307...\n",
            "Extracting features from 0hyZ__3YhZc_341_348...\n",
            "Extracting features from 0hyZ__3YhZc_352_356...\n",
            "Extracting features from 0hyZ__3YhZc_364_370...\n",
            "Extracting features from 0hyZ__3YhZc_380_384...\n",
            "Extracting features from 0hyZ__3YhZc_388_394...\n",
            "Extracting features from 0hyZ__3YhZc_410_417...\n",
            "Extracting features from 0hyZ__3YhZc_418_424...\n",
            "Extracting features from 0hyZ__3YhZc_485_490...\n",
            "Extracting features from 0hyZ__3YhZc_562_568...\n",
            "Extracting features from 0hyZ__3YhZc_575_580...\n",
            "Extracting features from 0hyZ__3YhZc_598_603...\n",
            "Extracting features from 0hyZ__3YhZc_632_637...\n",
            "Extracting features from 0k1Ak8aTMVI_4_12...\n",
            "Extracting features from 0lh_UWF9ZP4_103_110...\n",
            "Extracting features from 0lh_UWF9ZP4_138_145...\n",
            "Extracting features from 0lh_UWF9ZP4_148_155...\n",
            "Extracting features from 0lh_UWF9ZP4_157_160...\n",
            "Extracting features from 0lh_UWF9ZP4_165_170...\n",
            "Extracting features from 0lh_UWF9ZP4_174_178...\n",
            "Extracting features from 0lh_UWF9ZP4_178_182...\n",
            "Extracting features from 0lh_UWF9ZP4_183_190...\n",
            "Extracting features from 0lh_UWF9ZP4_191_197...\n",
            "Extracting features from 0lh_UWF9ZP4_199_207...\n",
            "Extracting features from 0lh_UWF9ZP4_215_226...\n",
            "Extracting features from 0lh_UWF9ZP4_21_26...\n",
            "Extracting features from 0lh_UWF9ZP4_27_31...\n",
            "Extracting features from 0lh_UWF9ZP4_38_46...\n",
            "Extracting features from 0lh_UWF9ZP4_50_60...\n",
            "Extracting features from 0lh_UWF9ZP4_62_69...\n",
            "Extracting features from 0lh_UWF9ZP4_71_77...\n",
            "Extracting features from 0lh_UWF9ZP4_79_82...\n",
            "Extracted features for 56 videos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save features dictionary for later use\n",
        "with open(\"video_features.pkl\", \"wb\") as f:\n",
        "    pickle.dump(video_features, f)\n",
        "\n",
        "print(\"Saved features to video_features.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KST6SzIRpqYC",
        "outputId": "40c728cc-dbe9-4c5a-df05-0821f3bcc6d4"
      },
      "id": "KST6SzIRpqYC",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features to video_features.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOKENIZE"
      ],
      "metadata": {
        "id": "HKUeKNdbwJQO"
      },
      "id": "HKUeKNdbwJQO"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Prepare all captions in one list\n",
        "all_captions = []\n",
        "for caps in captions_dict.values():\n",
        "    for cap in caps:\n",
        "        all_captions.append(f\"<start> {cap} <end>\")  # Add tokens\n",
        "\n",
        "# Fit tokenizer\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "# Max caption length\n",
        "max_length = max(len(caption.split()) for caption in all_captions)\n",
        "print(\"Max caption length:\", max_length)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2BQJ8NVwKXl",
        "outputId": "7b7a371d-7989-4c52-89e5-1bec7eb68121"
      },
      "id": "s2BQJ8NVwKXl",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 858\n",
            "Max caption length: 34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean and tokenize captions\n",
        "def clean_caption(caption):\n",
        "    caption = caption.lower().strip()\n",
        "    return f\"startseq {caption} endseq\"\n",
        "\n",
        "all_captions = []\n",
        "for caps in captions.values():\n",
        "    for cap in caps:\n",
        "        all_captions.append(clean_caption(cap))\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_length = max(len(c.split()) for c in all_captions)\n"
      ],
      "metadata": {
        "id": "y9V1VSqrp7yY"
      },
      "id": "y9V1VSqrp7yY",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING SEQUENCE"
      ],
      "metadata": {
        "id": "9HC84qR9wP6K"
      },
      "id": "9HC84qR9wP6K"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "# Clean and add special tokens\n",
        "def clean_caption(caption):\n",
        "    caption = caption.lower().strip()\n",
        "    return f\"<start> {caption} <end>\"\n",
        "\n",
        "# Create training sequences\n",
        "def create_sequences(tokenizer, max_length, descriptions, features, vocab_size):\n",
        "    X1, X2, y = [], [], []\n",
        "    for key, caps in descriptions.items():\n",
        "        if key not in features:\n",
        "            continue\n",
        "        feature = features[key]\n",
        "        for cap in caps:\n",
        "            cap = clean_caption(cap)\n",
        "            seq = tokenizer.texts_to_sequences([cap])[0]\n",
        "            for i in range(1, len(seq)):\n",
        "                in_seq, out_seq = seq[:i], seq[i]\n",
        "                in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]\n",
        "                out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                X1.append(feature)\n",
        "                X2.append(in_seq)\n",
        "                y.append(out_seq)\n",
        "    return np.array(X1), np.array(X2), np.array(y)\n",
        "\n",
        "# Define the model\n",
        "def define_model(vocab_size, max_length):\n",
        "    # Feature extractor (video)\n",
        "    inputs1 = Input(shape=(4096,))\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(256, activation=\"relu\")(fe1)\n",
        "\n",
        "    # Sequence processor (caption)\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=False)(inputs2)  # mask_zero=False for compatibility\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    se3 = LSTM(256)(se2)\n",
        "\n",
        "    # Decoder (fusion)\n",
        "    decoder1 = add([fe2, se3])\n",
        "    decoder2 = Dense(256, activation=\"relu\")(decoder1)\n",
        "    outputs = Dense(vocab_size, activation=\"softmax\")(decoder2)\n",
        "\n",
        "    # Final model\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "K1d-nmGj38s4"
      },
      "id": "K1d-nmGj38s4",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = define_model(vocab_size, max_length)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "3bmRXZrSwTgn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "09e20a58-ec7d-4076-ea40-3c19a4ce4a9a"
      },
      "id": "3bmRXZrSwTgn",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_6       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m219,648\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │  \u001b[38;5;34m1,048,832\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m525,312\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                     │                   │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m858\u001b[0m)       │    \u001b[38;5;34m220,506\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_6       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">219,648</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,048,832</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                     │                   │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">858</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">220,506</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,080,090\u001b[0m (7.93 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,080,090</span> (7.93 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,080,090\u001b[0m (7.93 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,080,090</span> (7.93 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit([X1, X2], y, epochs=10, batch_size=64)\n",
        "model.save(\"video_caption_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlCHYOM7qEa_",
        "outputId": "4dc63afe-71c5-426c-f5b7-85a8c797c2c3"
      },
      "id": "NlCHYOM7qEa_",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 274ms/step - loss: 4.7238\n",
            "Epoch 2/10\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 271ms/step - loss: 3.3455\n",
            "Epoch 3/10\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 265ms/step - loss: 2.8560\n",
            "Epoch 4/10\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 267ms/step - loss: 2.6090\n",
            "Epoch 5/10\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 263ms/step - loss: 2.4532\n",
            "Epoch 6/10\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 269ms/step - loss: 2.3592\n",
            "Epoch 7/10\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 266ms/step - loss: 2.2366\n",
            "Epoch 8/10\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 264ms/step - loss: 2.2212\n",
            "Epoch 9/10\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 275ms/step - loss: 2.1172\n",
            "Epoch 10/10\n",
            "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 265ms/step - loss: 2.0779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "caption prediction"
      ],
      "metadata": {
        "id": "CgF5EkrC5Zdo"
      },
      "id": "CgF5EkrC5Zdo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "greedy search"
      ],
      "metadata": {
        "id": "TyFIUtJG6TTC"
      },
      "id": "TyFIUtJG6TTC"
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧾 Caption prediction\n",
        "def generate_caption(model, tokenizer, photo, max_length):\n",
        "    in_text = \"startseq\"\n",
        "    for _ in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo.reshape((1,4096)), sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = tokenizer.index_word.get(yhat)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += \" \" + word\n",
        "        if word == \"endseq\":\n",
        "            break\n",
        "    return in_text.replace(\"startseq\", \"\").replace(\"endseq\", \"\").strip()\n"
      ],
      "metadata": {
        "id": "A6piThgmqpX_"
      },
      "id": "A6piThgmqpX_",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model(\"video_caption_model.h5\")\n",
        "\n",
        "# Load tokenizer and video features (if needed again)\n",
        "with open(\"video_features.pkl\", \"rb\") as f:\n",
        "    video_features = pickle.load(f)\n",
        "\n",
        "# Let's choose a random video to test\n",
        "import random\n",
        "sample_video = random.choice(list(video_features.keys()))\n",
        "print(f\" Testing video: {sample_video}\")\n",
        "\n",
        "# Extract feature\n",
        "feature = video_features[sample_video]\n",
        "\n",
        "# Generate caption\n",
        "predicted_caption = generate_caption(model, tokenizer, feature, max_length)\n",
        "print(f\" Generated caption: {predicted_caption}\")\n",
        "\n",
        "# Show reference captions (ground truth)\n",
        "print(\"\\n Ground truth captions:\")\n",
        "for cap in captions[sample_video]:\n",
        "    print(\"-\", cap)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHOH3bIkqsmF",
        "outputId": "f9fa471a-767d-4f84-9297-3d902ff8d4bf"
      },
      "id": "KHOH3bIkqsmF",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Testing video: -_hbPLsZvvo_5_8\n",
            " Generated caption: dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog is is is barking end end end end barking end barking end end end barking\n",
            "\n",
            " Ground truth captions:\n",
            "- a dog is chewing\n",
            "- a dog appears to be talking next to a woman cook\n",
            "- a dog is barking\n",
            "- a dog is barking\n",
            "- a dog is barking\n",
            "- a dog is barking\n",
            "- a dog is chewing food\n",
            "- a dog is barking\n",
            "- a dog is eating\n",
            "- a dog barks\n",
            "- a dog is chewing something\n",
            "- a dog is barking\n",
            "- a dog is barking\n",
            "- a dog is chewing on food\n",
            "- the dog happily ate the sushi\n",
            "- the dog is eating\n",
            "- a dog barks\n",
            "- the dog ate the sushi\n",
            "- a dog is barking\n",
            "- a puppy is barking\n",
            "- a dog barking and cooking with her master in the kitchen\n",
            "- a women cooked a tasty food with her dog\n",
            "- a dog is craying\n",
            "- learn to make an easy japanese lunch bento\n",
            "- a dog is chewing\n",
            "- a woman making a bento\n",
            "- a dog is eating\n",
            "- a dog is barking\n",
            "- the dog is sitting\n",
            "- a lady is cooking with dog\n",
            "- a dog is barking\n",
            "- a cooking show\n",
            "- a dog is talking\n",
            "- a grey poodle barking loudly\n",
            "- a dog is barking\n",
            "- a dog barging\n",
            "- a dog is eating\n",
            "- someone is cooking\n",
            "- a lady cooking in her kitchen with her dog\n",
            "- a dog barks and then a lady in a kitchen bows\n",
            "- a dog is chewing\n",
            "- a man cooking his kichen\n",
            "- a person is making bento japanese boxed lunch\n",
            "- the dog is eating\n",
            "- a dog is barking\n",
            "- a dog barks\n",
            "- she is cooking\n",
            "- the cooking with dog\n",
            "- a gray poodle is barking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "beam search"
      ],
      "metadata": {
        "id": "OGY_9C1u6QKf"
      },
      "id": "OGY_9C1u6QKf"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption_beam(model, tokenizer, video_feat, max_length, beam_width=3):\n",
        "    start = [\"startseq\"]\n",
        "    sequences = [(start, 0.0)]  # (caption so far, log probability)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        all_candidates = []\n",
        "        for seq, score in sequences:\n",
        "            if seq[-1] == \"endseq\":\n",
        "                all_candidates.append((seq, score))\n",
        "                continue\n",
        "            # Convert to sequence\n",
        "            sequence = tokenizer.texts_to_sequences([\" \".join(seq)])[0]\n",
        "            sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n",
        "            yhat = model.predict([video_feat.reshape((1, 4096)), sequence], verbose=0)\n",
        "            # Get top candidates\n",
        "            top_indices = np.argsort(yhat[0])[-beam_width:]\n",
        "            for idx in top_indices:\n",
        "                word = tokenizer.index_word.get(idx)\n",
        "                if word:\n",
        "                    candidate = seq + [word]\n",
        "                    log_prob = np.log(yhat[0][idx] + 1e-10)\n",
        "                    all_candidates.append((candidate, score + log_prob))\n",
        "        # Order by score\n",
        "        ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n",
        "        sequences = ordered[:beam_width]\n",
        "\n",
        "    final_caption = sequences[0][0]\n",
        "    return \" \".join(final_caption).replace(\"startseq\", \"\").replace(\"endseq\", \"\").strip()\n"
      ],
      "metadata": {
        "id": "aa55mxB46PIU"
      },
      "id": "aa55mxB46PIU",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model(\"video_caption_model.h5\")\n",
        "\n",
        "# Load tokenizer and video features (if needed again)\n",
        "with open(\"video_features.pkl\", \"rb\") as f:\n",
        "    video_features = pickle.load(f)\n",
        "\n",
        "# Let's choose a random video to test\n",
        "import random\n",
        "sample_video = random.choice(list(video_features.keys()))\n",
        "print(f\" Testing video: {sample_video}\")\n",
        "\n",
        "# Extract feature\n",
        "feature = video_features[sample_video]\n",
        "\n",
        "# Generate caption\n",
        "predicted_caption = generate_caption_beam(model, tokenizer, feature, max_length)\n",
        "print(f\" Generated caption: {predicted_caption}\")\n",
        "\n",
        "# Show reference captions (ground truth)\n",
        "print(\"\\n Ground truth captions:\")\n",
        "for cap in captions[sample_video]:\n",
        "    print(\"-\", cap)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_jxK70qFGf4",
        "outputId": "2cf56e72-d178-4317-d430-78cf32887114"
      },
      "id": "A_jxK70qFGf4",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Testing video: 05gNigkqfNU_25_34\n",
            " Generated caption: one is cutting potatoes end salad end end salad end end end end salad end end end end end end end end end end end end end end end end end end end end\n",
            "\n",
            " Ground truth captions:\n",
            "- a person cutting up potatoes\n",
            "- a chef cuts a potato\n",
            "- a person cuts potato wedges\n",
            "- a person is chopping potato slices with a knife\n",
            "- a person is chopping potatoes\n",
            "- a person is slicing some potato\n",
            "- a potato is being cut\n",
            "- a woman chops strips of raw potato into cubes with a knife\n",
            "- a woman is chopping a potato\n",
            "- potatoes are being chopped\n",
            "- someone chopped up a potatoe\n",
            "- someone dicing potatoes\n",
            "- someone is chopping potatoes\n",
            "- someone is chopping up the potatoe\n",
            "- someone is dicing potatoes\n",
            "- the person is dicing potatoes\n",
            "- slicing egg\n",
            "- teaching how to make country potato salad\n",
            "- a guy slicimg the eggs\n",
            "- a cooker is cutting a potato\n",
            "- some one is slicing potatoes\n",
            "- a man is cutting potatto\n",
            "- potatoes are being cut the proper way\n",
            "- a man is cuting some potate pieces\n",
            "- a man is cutting the eggs\n",
            "- a man is cutting potatoes\n",
            "- country women cutting country potato\n",
            "- a men making country potato salad\n",
            "- the man is dicing potatoes\n",
            "- a potato is being sliced\n",
            "- one person is explaining how to prepair potato salad\n",
            "- someone is chopping potatoes into cube sized pieces\n",
            "- a cooking tutorial on how to make potato salad\n",
            "- a man chops potatoes\n",
            "- a person is cutting potatoes\n",
            "- a man is cutting the potatoes in to pieces\n",
            "- a person is cutting a potato\n",
            "- someone cutting potatoes in chunks\n",
            "- a person is cutting a potato into pieces\n",
            "- a person is chopping up potatoes with a large knife\n",
            "- someone cuts potatoes\n",
            "- someone is dicing a potato\n",
            "- somebodi is slicing potatos\n",
            "- a person chops up hard boiled eggs\n",
            "- a short clip of showing how to make a tasty potato salad\n",
            "- a man is preparing the potato salad\n",
            "- someone is cutting up potatoes\n",
            "- a person is dicing potatoes\n",
            "- a woman is cooking her kichen\n",
            "- someone is chopping potatoes\n",
            "- the cook is dicing potatoes\n",
            "- a person is pealing potato\n",
            "- it is a method of preparing country potato salad recipe\n",
            "- someone is chopping potatoes\n",
            "- some potatoes are being chopped with a knife\n",
            "- a man is peal a potato\n",
            "- a person slicing potatoes\n",
            "- someone is cutting a potato\n",
            "- the person is peeling the potato and dicing the potato\n",
            "- the person is cutting the vegetables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluation"
      ],
      "metadata": {
        "id": "OhwbSjcn6N1h"
      },
      "id": "OhwbSjcn6N1h"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGT8HcuF6GDg",
        "outputId": "d9a6e9e3-88b4-4df8-e8b2-f7f8a728a79e"
      },
      "id": "IGT8HcuF6GDg",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "def evaluate_caption(reference_captions, predicted_caption):\n",
        "    \"\"\"Compares a predicted caption against references using BLEU and METEOR.\"\"\"\n",
        "    smooth = SmoothingFunction().method4\n",
        "\n",
        "    # Tokenize\n",
        "    references = [ref.split() for ref in reference_captions]\n",
        "    candidate = predicted_caption.split()\n",
        "\n",
        "    bleu1 = sentence_bleu(references, candidate, weights=(1.0, 0, 0, 0), smoothing_function=smooth)\n",
        "    bleu2 = sentence_bleu(references, candidate, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth)\n",
        "\n",
        "    print(f\"BLEU-1: {bleu1:.4f}\")\n",
        "    print(f\"BLEU-2: {bleu2:.4f}\")"
      ],
      "metadata": {
        "id": "6b9BJJ_C6IWx"
      },
      "id": "6b9BJJ_C6IWx",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OHO9H6JtFz64"
      },
      "id": "OHO9H6JtFz64",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2DbfFx7B4Et1"
      },
      "id": "2DbfFx7B4Et1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "xtras"
      ],
      "metadata": {
        "id": "wSfAy_PA4G8e"
      },
      "id": "wSfAy_PA4G8e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "split stuff"
      ],
      "metadata": {
        "id": "u7x3Hdkr-Gef"
      },
      "id": "u7x3Hdkr-Gef"
    },
    {
      "cell_type": "code",
      "source": [
        "caption_dict = {}\n",
        "\n",
        "with open(\"data/captions/mapped_train.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        filename, caption = line.strip().split(\" \", 1)  # split only once\n",
        "        if filename not in caption_dict:\n",
        "            caption_dict[filename] = []\n",
        "        caption_dict[filename].append(caption)\n",
        "\n",
        "print(f\"Loaded {len(caption_dict)} videos with captions.\")\n"
      ],
      "metadata": {
        "id": "TxDnUhC1-IpA"
      },
      "id": "TxDnUhC1-IpA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "feature extraction"
      ],
      "metadata": {
        "id": "tlQtxWG6-qH8"
      },
      "id": "tlQtxWG6-qH8"
    },
    {
      "cell_type": "code",
      "source": [
        "# 🖼️ Extract features from videos (frame sampling)\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import pickle\n",
        "\n",
        "# Load pre-trained VGG16 model\n",
        "base_model = VGG16(weights=\"imagenet\")\n",
        "cnn_model = Model(inputs=base_model.input, outputs=base_model.get_layer(\"fc2\").output)\n",
        "\n",
        "def extract_frames(video_path, num_frames=10):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error opening video file: {video_path}\")\n",
        "        return np.array([])\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_idxs = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "    for idx in frame_idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frame = cv2.resize(frame, (224, 224))\n",
        "            frame = img_to_array(frame)\n",
        "            frame = preprocess_input(frame)\n",
        "            frames.append(frame)\n",
        "    cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "def extract_video_features(video_path):\n",
        "    frames = extract_frames(video_path)\n",
        "    if frames.size == 0:\n",
        "        return None\n",
        "    features = cnn_model.predict(frames, verbose=0)\n",
        "    return np.mean(features, axis=0)\n"
      ],
      "metadata": {
        "id": "kkE6wE-4-rjL"
      },
      "id": "kkE6wE-4-rjL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training seq"
      ],
      "metadata": {
        "id": "rkBYs9ia4IMe"
      },
      "id": "rkBYs9ia4IMe"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_sequences(caption_dict, feature_path, tokenizer, max_length):\n",
        "    X1, X2, y = [], [], []\n",
        "\n",
        "    for video_name, captions in caption_dict.items():\n",
        "        try:\n",
        "            feature = np.load(f\"{feature_path}/{video_name}.npy\")\n",
        "        except:\n",
        "            continue  # skip missing videos\n",
        "\n",
        "        for caption in captions:\n",
        "            seq = tokenizer.texts_to_sequences([f\"<start> {caption} <end>\"])[0]\n",
        "            for i in range(1, len(seq)):\n",
        "                in_seq, out_seq = seq[:i], seq[i]\n",
        "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                out_seq = np.zeros(vocab_size)\n",
        "                out_seq[out_seq := out_seq] = 1.0  # one-hot encode\n",
        "\n",
        "                X1.append(feature)       # video features\n",
        "                X2.append(in_seq)        # input caption so far\n",
        "                y.append(out_seq)        # next word to predict\n",
        "\n",
        "    return np.array(X1), np.array(X2), np.array(y)\n",
        "\n",
        "# Use it:\n",
        "feature_path = r\"C:\\Users\\sweth\\Downloads\\archive (1).zip\\MSVD\\features\"\n",
        "X1, X2, y = create_sequences(caption_dict, feature_path, tokenizer, max_length)\n"
      ],
      "metadata": {
        "id": "V3AuA3MC4F3C"
      },
      "id": "V3AuA3MC4F3C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training sequences\n",
        "def create_sequences(tokenizer, max_length, descriptions, features):\n",
        "    X1, X2, y = [], [], []\n",
        "    for key, caps in descriptions.items():\n",
        "        if key not in features:\n",
        "            continue\n",
        "        feature = features[key]\n",
        "        for cap in caps:\n",
        "            cap = clean_caption(cap)\n",
        "            seq = tokenizer.texts_to_sequences([cap])[0]\n",
        "            for i in range(1, len(seq)):\n",
        "                in_seq, out_seq = seq[:i], seq[i]\n",
        "                # pad sequences to the right instead of left\n",
        "                in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]\n",
        "                out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                X1.append(feature)\n",
        "                X2.append(in_seq)\n",
        "                y.append(out_seq)\n",
        "    return np.array(X1), np.array(X2), np.array(y)\n",
        "\n",
        "X1, X2, y = create_sequences(tokenizer, max_length, captions, video_features)\n",
        "\n",
        "# Define the model\n",
        "def define_model(vocab_size, max_length):\n",
        "    inputs1 = Input(shape=(4096,))\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(256, activation=\"relu\")(fe1)\n",
        "\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=False)(inputs2) # Set mask_zero to False\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    se3 = LSTM(256)(se2)\n",
        "\n",
        "    decoder1 = add([fe2, se3])\n",
        "    decoder2 = Dense(256, activation=\"relu\")(decoder1)\n",
        "    outputs = Dense(vocab_size, activation=\"softmax\")(decoder2)\n",
        "\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
        "    return model\n",
        "\n",
        "model = define_model(vocab_size, max_length)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ovj_osUNB7fO"
      },
      "id": "ovj_osUNB7fO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}